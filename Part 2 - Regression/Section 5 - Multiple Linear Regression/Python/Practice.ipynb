{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b448569-21ba-4619-9b32-46f39ce259d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Handling categorical values\n",
    "- To Handle categorical values we need to create a Column\n",
    "    - insert 1 if the categorical data present in that row in referenced categorical column\n",
    "    - if the categorical data not present in there insert 0 in those categorical columns\n",
    "    - We converted categorical data into numarical data\n",
    "    - ## this data is called Dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df27c7-acdc-4406-adb7-635fd7ece7e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "- Data columns that we had\n",
    "    - Profit , R&D Spend , Admin , Marketing , state( states are categorical Data)\n",
    "- in this we have to figure out the profits by using the sent amout on the different fields\n",
    "    - R&D Spend , Admin , Marketing , state\n",
    "- Equation = ```y=b0+b1*X1+b2*X2+b3*X3+(+b4*D1 (Categorical value))```\n",
    "- Profit is dependent variable and rest of them are independent variables\n",
    "- profit = Y\n",
    "- b0 = constent\n",
    "- b1*X1 R&D sent X1 is the amount that spent on the R&D\n",
    "- b2*X2 R&D sent X2 is the amount that spent on the Admin\n",
    "- b3*X3 R&D sent X3 is the amount that spent on the Marketing\n",
    "- All X1to X3 are the potential variable that predictors for the data \n",
    "    - some are importent and some are not \n",
    "    - if we add all data into the model it wont be reliable and it will get garbage\n",
    "- In the 4th Column we got categorical data that refers to the states\n",
    "    - to handle the categorical data we splitted each categorical data into seperate columns so that we got two different values New York and California\n",
    "        - if the new york present in the state column we add 1 to the newly created New york column and in the remening columns we insert 0 in to and vise-versa\n",
    "        - So we successfully converted a Categorical data into a numarical data\n",
    "        - because we had only two different categorical data we can use only one column for the categorical data insted of the `New York` and `California` columns if the value is '1' we refernced that to `New York` and If the value if '0' we refernced that to `California`\n",
    "        - If we iclude second column he new Equation is ```y=b0+b1*X1+b2*X2+b3*X3+(+b4*D1+b5*D2 (Categorical value))```\n",
    "            - in the above case the model will be in multi collienearity \n",
    "            - because of that the model will never distinguse between the two categorical data\n",
    "                - for california = ```b3*D1+b5*D2``` as D1 is zero remaining is ```0+b5*D2```\n",
    "                - for New York = ```b3*D1+b5*D2``` as D2 is zero remaining is ```b4*D1+0```\n",
    "                - because of that it wont work properly\n",
    " - b4*D1 (D1 i s the newly created Dummy variable for the Categorical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae6ab4-ff5d-44f4-8135-d2c11d657d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Methods of building the model\n",
    "- All in\n",
    "- Backward selection # Also called as stepwise Regression\n",
    "- Forward selection # Also called as stepwise Regression\n",
    "- Bidirectional Elimination # Also called as stepwise Regression\n",
    "- Score Comparison\n",
    "\n",
    "### All in\n",
    "- All the data variable(Columns) should in when preparing the model\n",
    "- If we fully knowldge in those variable that it will effect on the data then we should inclde the data \n",
    "- if there is order to use all variables in the model by superior or company said it should include\n",
    "- While peparing for the backward elimention\n",
    "\n",
    "### Backward Elimention\n",
    "- Step 1 - Select a significance level to stay in the model ex:- Statistical Significance level SL = 0.005 \n",
    "- Step 2 - Fit the full model with all possible Predictors\n",
    "- Step 3 - Consider the predictor with the highest P-value. if P>Sl goto step 4. other wise goto FIN(Finish The model is Ready)\n",
    "- Step 4 - Remove pridictor\n",
    "- Step 5 - fit modelwithout this variable\n",
    "    - After Step 5 go to Step 3 and start again checking\n",
    "    \n",
    "### Forward Selection\n",
    "- Step 1 Select a significance lvelto enter the model (eg SL = 0.05)\n",
    "- step 2 Fit all simple regression models y ~ Xn Select the onewith the lowest P-value\n",
    "- Step 3 keep this variable and fit all possible models with one extra predictor added to the one(s) you already have\n",
    "- Step 4 Consider the predictor with the lowest P-value. if P<SL goto Step 3, otherwise go to FIN(Finish The model is Ready) - Keep the previous model\n",
    "\n",
    "### Bidirectional Elimination\n",
    "- Step 1 Select a significance level to enter and stay in that model\n",
    "    - Ex SLENTER = 0.05, SLSTAY = 0.05\n",
    "- Step 2 Perform the next step of the forward selection (New Variables must have P < SLENTER to enter)\n",
    "- Step 3 Perform all steps of Backward Elimination (Old variables must have P < SLSTAY to stay )\n",
    "- Step 4 No new variables can enter and no old variables can exit\n",
    "    - FIN(Finish The model is Ready)\n",
    "    \n",
    "### ALL mpossible models -> Its a very Resource consuming process\n",
    "- Step 1 Select a criterion of goodness of fit (Ex akaike Criterion )\n",
    "- Step 2 Construct all possible Regression Models ((2^N)-1) where N is No of columns(AKA variables)\n",
    "- Step 3 Select the one with the best criterion\n",
    "    - FIN(Finish The model is Ready)\n",
    "   \n",
    "In all these models Backward Elemenation is the fastest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac5b84-de2a-490c-afd9-e150bb476bf0",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802799b-f127-4211-8d2a-39cd8bc097a4",
   "metadata": {
    "colab_type": "text",
    "id": "pOyqYHTk_Q57"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a722c974-a82b-4773-9518-25a6ba3be99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a42b1-4e00-49b5-860b-e9b90267cba9",
   "metadata": {
    "colab_type": "text",
    "id": "vgC61-ah_WIz",
    "tags": []
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1036d2-ed4d-4eaf-aacb-8e3bea7031a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('50_Startups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6eeffacc-0504-4602-b14b-b7cbe7e6508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "# We spit hthe data into two parts one is to be predicted value and anothe on is values should need to predict the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d17f2e56-61e9-4241-b3ae-1525273798f4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165349.2</td>\n",
       "      <td>136897.8</td>\n",
       "      <td>471784.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162597.7</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>131876.9</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86419.7</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.3</td>\n",
       "      <td>298664.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72107.6</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.3</td>\n",
       "      <td>35534.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116983.8</td>\n",
       "      <td>45173.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2          3          4          5\n",
       "0   0.0  0.0  1.0   165349.2   136897.8   471784.1\n",
       "1   1.0  0.0  0.0   162597.7  151377.59  443898.53\n",
       "2   0.0  1.0  0.0  153441.51  101145.55  407934.54\n",
       "3   0.0  0.0  1.0  144372.41  118671.85  383199.62\n",
       "4   0.0  1.0  0.0  142107.34   91391.77  366168.42\n",
       "5   0.0  0.0  1.0   131876.9   99814.71  362861.36\n",
       "6   1.0  0.0  0.0  134615.46  147198.87  127716.82\n",
       "7   0.0  1.0  0.0  130298.13  145530.06  323876.68\n",
       "8   0.0  0.0  1.0  120542.52  148718.95  311613.29\n",
       "9   1.0  0.0  0.0  123334.88  108679.17  304981.62\n",
       "10  0.0  1.0  0.0  101913.08  110594.11  229160.95\n",
       "11  1.0  0.0  0.0  100671.96   91790.61  249744.55\n",
       "12  0.0  1.0  0.0   93863.75  127320.38  249839.44\n",
       "13  1.0  0.0  0.0   91992.39  135495.07  252664.93\n",
       "14  0.0  1.0  0.0  119943.24  156547.42  256512.92\n",
       "15  0.0  0.0  1.0  114523.61  122616.84  261776.23\n",
       "16  1.0  0.0  0.0   78013.11  121597.55  264346.06\n",
       "17  0.0  0.0  1.0   94657.16  145077.58  282574.31\n",
       "18  0.0  1.0  0.0   91749.16  114175.79  294919.57\n",
       "19  0.0  0.0  1.0    86419.7  153514.11        0.0\n",
       "20  1.0  0.0  0.0   76253.86   113867.3  298664.47\n",
       "21  0.0  0.0  1.0   78389.47  153773.43  299737.29\n",
       "22  0.0  1.0  0.0   73994.56  122782.75  303319.26\n",
       "23  0.0  1.0  0.0   67532.53  105751.03  304768.73\n",
       "24  0.0  0.0  1.0   77044.01   99281.34  140574.81\n",
       "25  1.0  0.0  0.0   64664.71  139553.16  137962.62\n",
       "26  0.0  1.0  0.0   75328.87  144135.98  134050.07\n",
       "27  0.0  0.0  1.0    72107.6  127864.55  353183.81\n",
       "28  0.0  1.0  0.0   66051.52  182645.56   118148.2\n",
       "29  0.0  0.0  1.0   65605.48  153032.06  107138.38\n",
       "30  0.0  1.0  0.0   61994.48  115641.28   91131.24\n",
       "31  0.0  0.0  1.0   61136.38  152701.92   88218.23\n",
       "32  1.0  0.0  0.0   63408.86  129219.61   46085.25\n",
       "33  0.0  1.0  0.0   55493.95  103057.49  214634.81\n",
       "34  1.0  0.0  0.0   46426.07  157693.92  210797.67\n",
       "35  0.0  0.0  1.0   46014.02   85047.44  205517.64\n",
       "36  0.0  1.0  0.0   28663.76  127056.21  201126.82\n",
       "37  1.0  0.0  0.0   44069.95   51283.14  197029.42\n",
       "38  0.0  0.0  1.0   20229.59   65947.93   185265.1\n",
       "39  1.0  0.0  0.0   38558.51   82982.09   174999.3\n",
       "40  1.0  0.0  0.0   28754.33  118546.05  172795.67\n",
       "41  0.0  1.0  0.0   27892.92   84710.77  164470.71\n",
       "42  1.0  0.0  0.0   23640.93   96189.63  148001.11\n",
       "43  0.0  0.0  1.0   15505.73   127382.3   35534.17\n",
       "44  1.0  0.0  0.0   22177.74  154806.14   28334.72\n",
       "45  0.0  0.0  1.0    1000.23  124153.04    1903.93\n",
       "46  0.0  1.0  0.0    1315.46  115816.21  297114.46\n",
       "47  1.0  0.0  0.0        0.0  135426.92        0.0\n",
       "48  0.0  0.0  1.0     542.05   51743.15        0.0\n",
       "49  1.0  0.0  0.0        0.0   116983.8   45173.06"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fbcbee-09d7-4229-9c13-b862bab11d9e",
   "metadata": {},
   "source": [
    "In the given data we have to prepare the model to predictthe profit using information where they invested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eba376b-a79d-4a8e-a8c7-bee536b89b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['New York', 'California', 'Florida'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['State'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7005dbd-8045-42b6-8f3b-f48fcb11c084",
   "metadata": {},
   "source": [
    "- We have total 3 different categiory\n",
    "- to fix this we need to apply one hot encoding for this Independent variables(Column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4744e9-dd5d-44e1-a21c-c04c55745a0d",
   "metadata": {
    "colab_type": "text",
    "id": "VadrvE7s_lS9"
   },
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "604451e3-28ba-4ccc-b914-5114a20fe1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b3971-107c-4703-b14b-527745452137",
   "metadata": {
    "colab_type": "text",
    "id": "WemVnqgeA70k"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60cfcb20-4bb9-48bc-b4df-93051ffc297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting also take careof the dummy vaiable  \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e180d-9003-46ac-bd7e-4ee3ffa01d6b",
   "metadata": {
    "colab_type": "text",
    "id": "k-McZVsQBINc"
   },
   "source": [
    "## Training the Multiple Linear Regression model on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12d8bc49-2192-4d96-b851-dff7b5c228d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor=LinearRegression()\n",
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa9363e-436a-4591-a476-61b38943a64c",
   "metadata": {
    "colab_type": "text",
    "id": "xNkXL1YQBiBT"
   },
   "source": [
    "## Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc193c4-8abe-41c9-b73d-ad3be3551bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "y_Pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_Pred.reshape(len(y_Pred),1),y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517b1a8-5bb1-4308-8838-bd83ba347ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
